{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasa in c:\\users\\shifttech\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install farasa\n",
    "import re\n",
    "import string\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully âœ…\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "path = './data_set/fake_news_dataset.csv'\n",
    "fake_data = pd.read_csv(path, header=None,names=['text_column'])\n",
    "\n",
    "if fake_data.empty:\n",
    "    raise ValueError(\"The DataFrame is empty. Please check the CSV file âŒ\")\n",
    "else :\n",
    "    print(\"DataFrame loaded successfully âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø§ÙƒØªØ´Ø§Ù Ø¹Ù„Ø§Ø¬ Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³Ø±Ø·Ø§Ù† Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø­ÙƒÙˆÙ…Ø© Ø¯ÙˆÙ„Ø© Ø¹Ø±Ø¨ÙŠØ© ØªØ¹Ù„Ù† Ø¹Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù…ÙˆØ§Ù„ Ù…Ø¬Ø§Ù†Ù‹Ø§ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø±Ø¬Ù„ ÙŠØ´Ø±Ø¨ Ù…Ø§Ø¡Ù‹ ÙÙ‚Ø· Ù„Ù…Ø¯Ø© 365 ÙŠÙˆÙ…Ù‹Ø§ ÙˆÙŠØ¨Ù‚Ù‰ Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¹Ù„Ù…Ø§Ø¡ ÙŠØ¤ÙƒØ¯ÙˆÙ† Ø£Ù† Ø§Ù„Ø£Ø±Ø¶ Ù…Ø³Ø·Ø­Ø© ÙˆÙ„ÙŠØ³Øª ÙƒØ±ÙˆÙŠØ©!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ ÙÙŠ Ø§Ù„Ø¨Ù„Ø§Ø¯ Ø¨Ø³Ø¨Ø¨ ØªØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø®!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_column\n",
       "0  Ø§ÙƒØªØ´Ø§Ù Ø¹Ù„Ø§Ø¬ Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³Ø±Ø·Ø§Ù† Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹!\n",
       "1  Ø­ÙƒÙˆÙ…Ø© Ø¯ÙˆÙ„Ø© Ø¹Ø±Ø¨ÙŠØ© ØªØ¹Ù„Ù† Ø¹Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù…ÙˆØ§Ù„ Ù…Ø¬Ø§Ù†Ù‹Ø§ ...\n",
       "2  Ø±Ø¬Ù„ ÙŠØ´Ø±Ø¨ Ù…Ø§Ø¡Ù‹ ÙÙ‚Ø· Ù„Ù…Ø¯Ø© 365 ÙŠÙˆÙ…Ù‹Ø§ ÙˆÙŠØ¨Ù‚Ù‰ Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©!\n",
       "3           Ø¹Ù„Ù…Ø§Ø¡ ÙŠØ¤ÙƒØ¯ÙˆÙ† Ø£Ù† Ø§Ù„Ø£Ø±Ø¶ Ù…Ø³Ø·Ø­Ø© ÙˆÙ„ÙŠØ³Øª ÙƒØ±ÙˆÙŠØ©!\n",
       "4     Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ ÙÙŠ Ø§Ù„Ø¨Ù„Ø§Ø¯ Ø¨Ø³Ø¨Ø¨ ØªØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø®!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_column'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning for Arabic\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove diacritics (tashkeel)\n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    \n",
    "    # Normalize Alef variations to Alef\n",
    "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
    "    \n",
    "    # Normalize Ya and Alef Maksura\n",
    "    text = re.sub(r'[ÙŠÙ‰]', 'ÙŠ', text)\n",
    "    \n",
    "    # Normalize Hamzas\n",
    "    text = re.sub(r'[Ø¤Ø¦]', 'Ø¡', text)\n",
    "    \n",
    "    # Remove non-Arabic characters except spaces and numbers\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s0-9]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization (Using Farasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         text_column  \\\n",
      "0  Ø§ÙƒØªØ´Ø§Ù Ø¹Ù„Ø§Ø¬ Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³Ø±Ø·Ø§Ù† Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹    \n",
      "1  Ø­ÙƒÙˆÙ…Ø© Ø¯ÙˆÙ„Ø© Ø¹Ø±Ø¨ÙŠØ© ØªØ¹Ù„Ù† Ø¹Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù…ÙˆØ§Ù„ Ù…Ø¬Ø§Ù† Ø§ ...   \n",
      "2       Ø±Ø¬Ù„ ÙŠØ´Ø±Ø¨ Ù…Ø§Ø¡ ÙÙ‚Ø· Ù„Ù…Ø¯Ø© ÙŠÙˆÙ… Ø§ ÙˆÙŠØ¨Ù‚Ù‰ Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©    \n",
      "3           Ø¹Ù„Ù…Ø§Ø¡ ÙŠØ¤ÙƒØ¯ÙˆÙ† Ø£Ù† Ø§Ù„Ø£Ø±Ø¶ Ù…Ø³Ø·Ø­Ø© ÙˆÙ„ÙŠØ³Øª ÙƒØ±ÙˆÙŠØ©    \n",
      "4     Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ ÙÙŠ Ø§Ù„Ø¨Ù„Ø§Ø¯ Ø¨Ø³Ø¨Ø¨ ØªØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø®    \n",
      "\n",
      "                                        cleaned_text  \n",
      "0      Ø§ÙƒØªØ´Ø§Ù Ø¹Ù„Ø§Ø¬ Ù†Ù‡Ø§Ø¦ÙŠ Ù„  Ø¬Ù…ÙŠØ¹ Ù†ÙˆØ¹ Ø§Ù„  Ø³Ø±Ø·Ø§Ù† Ø£Ø³Ø¨ÙˆØ¹  \n",
      "1  Ø­ÙƒÙˆÙ…  Ù‡ Ø¯ÙˆÙ„Ø©  Ù‡ Ø¹Ø±Ø¨ÙŠ  Ù‡ Ø£Ø¹Ù„Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„  Ù…Ø§Ù„ Ù…Ø¬Ø§...  \n",
      "2        Ø±Ø¬Ù„ Ø´Ø±Ø¨ Ù…Ø§Ø¡ Ù„  Ù…Ø¯  Ù‡ Ùˆ  Ø¨Ù‚ÙŠ Ø¨  ØµØ­  Ù‡ Ø¬ÙŠØ¯  Ù‡  \n",
      "3     Ø¹Ø§Ù„Ù… Ø£ÙƒØ¯  ÙˆÙ† Ø§Ù„  Ø£Ø±Ø¶ Ù…Ø³Ø·Ø­  Ù‡ Ùˆ  Ù„ÙŠØ³  Øª ÙƒØ±ÙˆÙŠ  Ù‡  \n",
      "4       Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„  Ù…Ø¯Ø±Ø³Ø© Ø§Ù„  Ø¨Ù„Ø¯ Ø¨  Ø³Ø¨Ø¨ ØªØºÙŠØ± Ø§Ù„  Ù…Ù†Ø§Ø®  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "stemmer = FarasaStemmer()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "\n",
    "# You can also load a stopwords list from a file if you have one.\n",
    "with open('./data_set/list.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    ARABIC_STOPWORDS = set(f.read().splitlines())\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize Alif (Ø£, Ø¥, Ø¢ â†’ Ø§) and Ta Marbuta (Ø© â†’ Ù‡)\n",
    "    text = text.replace(\"Ø£\", \"Ø§\").replace(\"Ø¥\", \"Ø§\").replace(\"Ø¢\", \"Ø§\").replace(\"Ø©\", \"Ù‡\")\n",
    "    # Tokenize & Stem using Farasa\n",
    "    tokens = segmenter.segment(text).split()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    final_tokens = [word for word in stemmed_tokens if word not in ARABIC_STOPWORDS]\n",
    "    \n",
    "    return \" \".join(final_tokens)\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "fake_data[\"cleaned_text\"] = fake_data[\"text_column\"].apply(clean_text)\n",
    "\n",
    "# Save preprocessed dataset\n",
    "fake_data.to_csv(\"preprocessed_dataset.csv\", index=False)\n",
    "\n",
    "print(fake_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  analyse morphologique With Farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "\n",
    "# Initialize Farasa POS Tagger\n",
    "farasa_pos = FarasaPOSTagger()\n",
    "\n",
    "# Load preprocessed dataset\n",
    "df_cleaned = pd.read_csv(\"./preprocessed_dataset.csv\")\n",
    "\n",
    "# Apply POS tagging to each sentence\n",
    "df_cleaned[\"pos_tagged\"] = df_cleaned[\"cleaned_text\"].apply(lambda text: farasa_pos.tag(text))\n",
    "\n",
    "# Print results in a readable format\n",
    "for i, sentence in enumerate(df_cleaned[\"pos_tagged\"]):\n",
    "    print(f\"ğŸ”¹ Sentence {i+1}:\")\n",
    "    for item in sentence:\n",
    "        if \"/\" in item:  # Ensure correct format\n",
    "            word, tag = item.rsplit(\"/\", 1)  # Split only on the last \"/\"\n",
    "            print(f\"{word} â†’ {tag}\")\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")  # Add a separator between sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convert TF-IDF Matrix to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Machine Learning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
