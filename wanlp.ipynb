{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install farasa\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Load dataset\n",
    "path = './data_set/train_set.csv'\n",
    "fake_data = pd.read_csv(path, header=None)\n",
    "\n",
    "if fake_data.empty:\n",
    "    raise ValueError(\"The DataFrame is empty. Please check the CSV file ❌\")\n",
    "else :\n",
    "    print(\"DataFrame loaded successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fake_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fake_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning for Arabic\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)# Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)# Remove HTML tags\n",
    "    arabic_diacritics = re.compile(r'[\\u064B-\\u065F]')\n",
    "    text = arabic_diacritics.sub('', text)# Remove diacritics (tashkeel)\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)  # Normalize Alif\n",
    "    text = re.sub(r'ة', 'ه', text)  # Normalize Teh Marbuta\n",
    "    text = re.sub(r'ى', 'ي', text)  # Normalize Ya\n",
    "    text = re.sub(r'[ؤئ]', 'ء', text) # Normalize Hamzas\n",
    "    # Remove non-Arabic characters except spaces and numbers\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s0-9]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[^ء-ي\\s]', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization (Using Farasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linguistic and Stylistic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load preprocessed dataset\n",
    "fake_data = pd.read_csv(\"preprocessed_100_rows.csv\", header=0)\n",
    "\n",
    "# Initialize sentiment analysis pipeline (for extracting alarmist tone or exaggeration)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"aubmindlab/bert-base-arabic-sentiment\")\n",
    "\n",
    "# Initialize AraBERT tokenizer and model for classification\n",
    "model_name = \"aubmindlab/bert-base-arabic\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 classes: reliable, doubtful, fake\n",
    "\n",
    "# Define a function for linguistic feature extraction\n",
    "def extract_linguistic_features(text):\n",
    "    # Sentiment analysis: alarmist or exaggerated language might have a certain sentiment\n",
    "    sentiment = sentiment_analyzer(text)\n",
    "    \n",
    "    # Add more feature extraction here (e.g., keyword-based detection, sentiment score)\n",
    "    return {\n",
    "        \"sentiment\": sentiment[0]['label'],  # 'LABEL_0', 'LABEL_1', or 'LABEL_2' based on sentiment\n",
    "    }\n",
    "\n",
    "# Define a function for AraBERT classification (reliable, doubtful, fake)\n",
    "def classify_text_with_arabert(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "    return prediction  # Class index: 0 (reliable), 1 (doubtful), 2 (fake)\n",
    "\n",
    "# Apply feature extraction and classification to the dataset\n",
    "fake_data[\"linguistic_features\"] = fake_data[\"cleaned_text\"].apply(extract_linguistic_features)\n",
    "fake_data[\"classification\"] = fake_data[\"cleaned_text\"].apply(classify_text_with_arabert)\n",
    "\n",
    "# Map the class indices to readable labels\n",
    "class_mapping = {0: \"reliable\", 1: \"doubtful\", 2: \"fake\"}\n",
    "fake_data[\"classification\"] = fake_data[\"classification\"].map(class_mapping)\n",
    "\n",
    "# Save the results to a new CSV\n",
    "fake_data.to_csv(\"preprocessed_with_classification.csv\", index=False)\n",
    "\n",
    "# Print the first few rows of the processed data\n",
    "print(fake_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Models for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching Models with LLMs Specialized in Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
