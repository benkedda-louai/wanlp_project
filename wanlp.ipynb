{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasa in c:\\users\\shifttech\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install farasa\n",
    "import re\n",
    "import string\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "path = './data_set/fake_news_dataset.csv'\n",
    "fake_data = pd.read_csv(path, header=None,names=['text_column'])\n",
    "\n",
    "if fake_data.empty:\n",
    "    raise ValueError(\"The DataFrame is empty. Please check the CSV file ❌\")\n",
    "else :\n",
    "    print(\"DataFrame loaded successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اكتشاف علاج نهائي لجميع أنواع السرطان خلال أسبوع!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حكومة دولة عربية تعلن عن توزيع الأموال مجانًا ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>رجل يشرب ماءً فقط لمدة 365 يومًا ويبقى بصحة جيدة!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>علماء يؤكدون أن الأرض مسطحة وليست كروية!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>إغلاق جميع المدارس في البلاد بسبب تغير المناخ!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_column\n",
       "0  اكتشاف علاج نهائي لجميع أنواع السرطان خلال أسبوع!\n",
       "1  حكومة دولة عربية تعلن عن توزيع الأموال مجانًا ...\n",
       "2  رجل يشرب ماءً فقط لمدة 365 يومًا ويبقى بصحة جيدة!\n",
       "3           علماء يؤكدون أن الأرض مسطحة وليست كروية!\n",
       "4     إغلاق جميع المدارس في البلاد بسبب تغير المناخ!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_column'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning for Arabic\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove diacritics (tashkeel)\n",
    "    text = re.sub(r'[\\u064B-\\u065F\\u0670]', '', text)\n",
    "    \n",
    "    # Normalize Alef variations to Alef\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    \n",
    "    # Normalize Ya and Alef Maksura\n",
    "    text = re.sub(r'[يى]', 'ي', text)\n",
    "    \n",
    "    # Normalize Hamzas\n",
    "    text = re.sub(r'[ؤئ]', 'ء', text)\n",
    "    \n",
    "    # Remove non-Arabic characters except spaces and numbers\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s0-9]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization (Using Farasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         text_column  \\\n",
      "0  اكتشاف علاج نهائي لجميع أنواع السرطان خلال أسبوع    \n",
      "1  حكومة دولة عربية تعلن عن توزيع الأموال مجان ا ...   \n",
      "2       رجل يشرب ماء فقط لمدة يوم ا ويبقى بصحة جيدة    \n",
      "3           علماء يؤكدون أن الأرض مسطحة وليست كروية    \n",
      "4     إغلاق جميع المدارس في البلاد بسبب تغير المناخ    \n",
      "\n",
      "                                        cleaned_text  \n",
      "0      اكتشاف علاج نهائي ل  جميع نوع ال  سرطان أسبوع  \n",
      "1  حكوم  ه دولة  ه عربي  ه أعلن توزيع ال  مال مجا...  \n",
      "2        رجل شرب ماء ل  مد  ه و  بقي ب  صح  ه جيد  ه  \n",
      "3     عالم أكد  ون ال  أرض مسطح  ه و  ليس  ت كروي  ه  \n",
      "4       إغلاق ال  مدرسة ال  بلد ب  سبب تغير ال  مناخ  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "stemmer = FarasaStemmer()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "\n",
    "# You can also load a stopwords list from a file if you have one.\n",
    "with open('./data_set/list.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    ARABIC_STOPWORDS = set(f.read().splitlines())\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize Alif (أ, إ, آ → ا) and Ta Marbuta (ة → ه)\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\").replace(\"ة\", \"ه\")\n",
    "    # Tokenize & Stem using Farasa\n",
    "    tokens = segmenter.segment(text).split()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    final_tokens = [word for word in stemmed_tokens if word not in ARABIC_STOPWORDS]\n",
    "    \n",
    "    return \" \".join(final_tokens)\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "fake_data[\"cleaned_text\"] = fake_data[\"text_column\"].apply(clean_text)\n",
    "\n",
    "# Save preprocessed dataset\n",
    "fake_data.to_csv(\"preprocessed_dataset.csv\", index=False)\n",
    "\n",
    "print(fake_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  analyse morphologique With Farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "\n",
    "# Initialize Farasa POS Tagger\n",
    "farasa_pos = FarasaPOSTagger()\n",
    "\n",
    "# Load preprocessed dataset\n",
    "df_cleaned = pd.read_csv(\"./preprocessed_dataset.csv\")\n",
    "\n",
    "# Apply POS tagging to each sentence\n",
    "df_cleaned[\"pos_tagged\"] = df_cleaned[\"cleaned_text\"].apply(lambda text: farasa_pos.tag(text))\n",
    "\n",
    "# Print results in a readable format\n",
    "for i, sentence in enumerate(df_cleaned[\"pos_tagged\"]):\n",
    "    print(f\"🔹 Sentence {i+1}:\")\n",
    "    for item in sentence:\n",
    "        if \"/\" in item:  # Ensure correct format\n",
    "            word, tag = item.rsplit(\"/\", 1)  # Split only on the last \"/\"\n",
    "            print(f\"{word} → {tag}\")\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")  # Add a separator between sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convert TF-IDF Matrix to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Machine Learning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
