{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "tqdm.pandas()\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "stemmer = FarasaStemmer()\n",
    "ner = FarasaNamedEntityRecognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the path for the data set here\n",
    "path = '../data_set/train_set.csv'\n",
    "path_2 = '../data_set/algerian_dialect_news.csv'\n",
    "df = pd.read_csv(path, encoding='utf-8', skiprows=range(1, 3001))\n",
    "df = df.dropna(subset=['text'])  # Drop rows where 'text' is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5069, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>youtube</td>\n",
       "      <td>ØªÙˆÙÙ‰ ÙˆÙ„Ø§ Ù„Ø¨Ø§Ø±Ø­ Ø¨ØµØ­ ØºÙŠØ± Ø§Ù„Ø®Ø¨Ø± Ù…Ø²Ø§Ù„ Ù…Ø§ØªÙ†Ø´Ø±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>paraphrased</td>\n",
       "      <td>Ù…ØªØ­Ø¯ÙŠÙ‹Ø§ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø³ÙƒØ§Ù† Ù„ÙŠÙ† ÙŠØ±ÙØ¶ÙˆÙ† Ù…ØºØ§Ø¯Ø±Ø© Ø§Ù„Ù…Ø¨Ø§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>ÙˆÙ‚Ø§Ù„Øª Ø§Ù„ØµÙŠÙ† Ø¨Ù„ÙŠ Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Ø³ÙŠØ© Ù„Ø§Ø²Ù…Ø§ Ø¨Ø§Ø´  Ø´Ø¨Ù‡ Ø§Ù„Ø¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>manual</td>\n",
       "      <td>Ù„Ø§Ù‚ØªØµØ§Ø¯ ØªØ¹ Ø§Ù„ØµÙŠÙ† Ø¨Ø¯Ø§ ÙŠÙƒØ¨Ø± ÙÙ„Ù‚Ø±Ù† Ù„Ø¹Ø´Ø±ÙŠÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>Ø§Ù„ØµÙŠÙ† Ø¹Ø·Ø§Øª  Ù…Ù„ÙŠØ§Ø± Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ø£ÙØ±ÙŠÙ‚ÙŠØ§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label       source                                               text\n",
       "0      1      youtube          ØªÙˆÙÙ‰ ÙˆÙ„Ø§ Ù„Ø¨Ø§Ø±Ø­ Ø¨ØµØ­ ØºÙŠØ± Ø§Ù„Ø®Ø¨Ø± Ù…Ø²Ø§Ù„ Ù…Ø§ØªÙ†Ø´Ø± \n",
       "1      1  paraphrased  Ù…ØªØ­Ø¯ÙŠÙ‹Ø§ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø³ÙƒØ§Ù† Ù„ÙŠÙ† ÙŠØ±ÙØ¶ÙˆÙ† Ù…ØºØ§Ø¯Ø±Ø© Ø§Ù„Ù…Ø¨Ø§...\n",
       "2      0   translated  ÙˆÙ‚Ø§Ù„Øª Ø§Ù„ØµÙŠÙ† Ø¨Ù„ÙŠ Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Ø³ÙŠØ© Ù„Ø§Ø²Ù…Ø§ Ø¨Ø§Ø´  Ø´Ø¨Ù‡ Ø§Ù„Ø¬...\n",
       "3      1       manual            Ù„Ø§Ù‚ØªØµØ§Ø¯ ØªØ¹ Ø§Ù„ØµÙŠÙ† Ø¨Ø¯Ø§ ÙŠÙƒØ¨Ø± ÙÙ„Ù‚Ø±Ù† Ù„Ø¹Ø´Ø±ÙŠÙ† \n",
       "4      0   translated   Ø§Ù„ØµÙŠÙ† Ø¹Ø·Ø§Øª  Ù…Ù„ÙŠØ§Ø± Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ø£ÙØ±ÙŠÙ‚ÙŠØ§"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'source', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null rows: 0\n",
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count null rows\n",
    "null_rows_count = df.isnull().sum().sum()\n",
    "print(f\"Number of null rows: {null_rows_count}\")\n",
    "\n",
    "# Count duplicated rows\n",
    "duplicated_rows_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicated rows: {duplicated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ›‘ Removing Stopwords: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [00:00<00:00, 234282.77it/s]\n",
      "ğŸ”¤ Normalizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [00:00<00:00, 77290.82it/s]\n",
      "ğŸ“Œ Segmenting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [4:00:17<00:00,  2.84s/it]  \n",
      "ğŸŒ± Stemming: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [3:59:39<00:00,  2.84s/it]  \n",
      "ğŸ“Œ POS Tagging: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [13:08:05<00:00,  9.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLP Preprocessing Complete! Ready for AraBERT tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Arabic Text Normalization\n",
    "def normalize_arabic(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                   # Remove HTML tags\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s,]\", \"\", text, flags=re.UNICODE)  # Remove emojis\n",
    "    # Normalize Arabic text\n",
    "    text = re.sub(r\"[Ø¥Ø£Ø¢Ù±]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"[ÙÙÙÙ‘Ù’ÙÙŒÙ‹]\", \"\", text)  # Remove diacritics\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # Remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Load custom Arabic stopwords\n",
    "with open(\"../data_set/algerian_arabic_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(word.strip() for word in f.readlines())\n",
    "# -------------------------\n",
    "# Text Segmentation (Sentence Splitting)\n",
    "def segment_text(text):\n",
    "    # Farasa returns segmented text with morphological boundaries marked\n",
    "    segmented = segmenter.segment(text)\n",
    "    # Return as is or with your custom separator\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization (Root Extraction)\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join(stemmer.stem(text))\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging\n",
    "def pos_tag_text(text):\n",
    "    return \" \".join(pos_tagger.tag(text))\n",
    "\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# Stopword Removal\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
    "\n",
    "# -------------------------\n",
    "# Text Segmentation using Farasa\n",
    "def segment_text(text):\n",
    "    segmented = segmenter.segment(text)\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization using Farasa\n",
    "def lemmatize_text(text):\n",
    "    stemmed = stemmer.stem(text)\n",
    "    return stemmed\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging using Farasa\n",
    "def pos_tag_text(text):\n",
    "    tagged = pos_tagger.tag(text)\n",
    "    return tagged\n",
    "# -----------------------\n",
    "\n",
    "# Apply stopword removal\n",
    "tqdm.pandas(desc=\"ğŸ›‘ Removing Stopwords\")\n",
    "df[\"cleaned_text\"] = df[\"text\"].progress_apply(remove_stopwords)\n",
    "\n",
    "# Apply normalization\n",
    "tqdm.pandas(desc=\"ğŸ”¤ Normalizing\")\n",
    "df[\"normalized_text\"] = df[\"cleaned_text\"].progress_apply(normalize_arabic)\n",
    "\n",
    "# Apply segmentation\n",
    "tqdm.pandas(desc=\"ğŸ“Œ Segmenting\")\n",
    "df[\"segmented_text\"] = df[\"normalized_text\"].progress_apply(segment_text)\n",
    "\n",
    "# Apply stemming/lemmatization and convert to token list\n",
    "tqdm.pandas(desc=\"ğŸŒ± Stemming\")\n",
    "df[\"stemmed_tokens\"] = df[\"normalized_text\"].progress_apply(lambda text: stemmer.stem(text))\n",
    "df[\"stemmed_tokens\"] = df[\"stemmed_tokens\"].apply(lambda text: text.split())\n",
    "\n",
    "# Apply POS tagging and convert to token list\n",
    "tqdm.pandas(desc=\"ğŸ“Œ POS Tagging\")\n",
    "df[\"pos_tags\"] = df[\"normalized_text\"].progress_apply(lambda text: pos_tagger.tag(text))\n",
    "df[\"pos_tags\"] = df[\"pos_tags\"].apply(lambda text: text.split())\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ… NLP Preprocessing Complete! Ready for AraBERT tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>segmented_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>translated</td>\n",
       "      <td>Ø´Ù‰ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹Ø§ ØªØ¬Ø±Ø¨...</td>\n",
       "      <td>Ø´Ù‰ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹Ø§ ØªØ¬Ø±Ø¨...</td>\n",
       "      <td>Ø´ÙŠ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹Ø§ ØªØ¬Ø±Ø¨...</td>\n",
       "      <td>Ø´ÙŠ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„+ØªØ¹Ø§Ù…Ù„ Ø¨+Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹+Ø§ Øª...</td>\n",
       "      <td>['Ø´ÙŠ', 'Ø¨ÙˆØªÙŠÙ†', 'ÙˆØ§ÙÙ‚', 'Ø¹Ù„Ø§', 'ØªØ¹Ø§Ù…Ù„', 'Ø´ÙƒÙ„',...</td>\n",
       "      <td>['S/S', 'Ø´ÙŠ/V', 'Ø¨ÙˆØªÙŠÙ†/NOUN-MS', 'ÙŠÙˆØ§ÙÙ‚/V', 'Ø¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>youtube</td>\n",
       "      <td>Ù„ÙƒØ°Ø¨ ÙÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¯Ù… Ø¨Ø±ÙƒØ§Ùˆ Ù…Ù† Ù„ÙƒØ°Ø¨</td>\n",
       "      <td>Ù„ÙƒØ°Ø¨ ÙÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¯Ù… Ø¨Ø±ÙƒØ§Ùˆ Ù„ÙƒØ°Ø¨</td>\n",
       "      <td>Ù„ÙƒØ°Ø¨ ÙÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¯Ù… Ø¨Ø±ÙƒØ§Ùˆ Ù„ÙƒØ°Ø¨</td>\n",
       "      <td>Ù„+ÙƒØ°Ø¨ ÙÙˆØ¬+Ù‡ Ø¨+Ù†Ø§Ø¯Ù… Ø¨+Ø±ÙƒØ§Ùˆ Ù„+ÙƒØ°Ø¨</td>\n",
       "      <td>['ÙƒØ°Ø¨', 'ÙÙˆØ¬', 'Ù†Ø§Ø¯Ù…', 'Ø±ÙƒØ§Ùˆ', 'ÙƒØ°Ø¨']</td>\n",
       "      <td>['S/S', 'Ù„+/PREP', 'ÙƒØ°Ø¨/NOUN-MS', 'ÙÙˆØ¬/NOUN-MS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>youtube</td>\n",
       "      <td>Ø§Ù„Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ùˆ Ù„Ø§ Ø­...</td>\n",
       "      <td>Ø§Ù„Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ø­ÙˆÙ„Ø§ Ù‚...</td>\n",
       "      <td>Ø§Ù„Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ø­ÙˆÙ„Ø§ Ù‚...</td>\n",
       "      <td>Ø§Ù„+Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„+Ø¹Ø§Ù„Ù…+ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ø­ÙˆÙ„...</td>\n",
       "      <td>['Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡', 'Ø±Ø¨', 'Ø¹Ø§Ù„Ù…', 'Ø¨Ø³Ù…', 'Ø§Ù„Ù„Ù‡', 'Ù…Ø´Ø§Ø¡...</td>\n",
       "      <td>['S/S', 'Ø§Ù„+', 'Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡/DET+NOUN-MS', 'Ø±Ø¨/NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‡Ø¬Ù…Ø§Øª Ø¹Ù„Ù‰ ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠØ© Ø³Ø§Ø­Ù„...</td>\n",
       "      <td>Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‡Ø¬Ù…Ø§Øª ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠØ© Ø³Ø§Ø­Ù„ Ø§Ù„Ø¹Ø§Ø¬</td>\n",
       "      <td>Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‡Ø¬Ù…Ø§Øª ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠÙ‡ Ø³Ø§Ø­Ù„ Ø§Ù„Ø¹Ø§Ø¬</td>\n",
       "      <td>Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„+Ù‡Ø¬Ù…+Ø§Øª ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠ+Ù‡ Ø³Ø§Ø­Ù„ ...</td>\n",
       "      <td>['Ø­Ù„ÙŠÙ', 'Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ', 'ÙˆØ±Ø§Ø¡', 'Ù‡Ø¬Ù…Ø©', 'ÙˆØ²ÙŠØ±', 'Ø¯Ø§...</td>\n",
       "      <td>['S/S', 'Ø­Ù„ÙØ§Ø¡/NOUN-MP', 'Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ/NOUN-MS', 'ÙˆØ±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚Ù‡Ø§ ÙØ§Ù„ÙØªØ±Ø© Ø§Ù„ÙŠ ...</td>\n",
       "      <td>Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚Ù‡Ø§ ÙØ§Ù„ÙØªØ±Ø© ØªØ³Ø¨Ù‚ Ø§Ù†Øª...</td>\n",
       "      <td>Ø§Ù„Ø³ÙŠØ·Ø±Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚Ù‡Ø§ ÙØ§Ù„ÙØªØ±Ù‡ ØªØ³Ø¨Ù‚ Ø§Ù†Øª...</td>\n",
       "      <td>Ø§Ù„+Ø³ÙŠØ·Ø±Ù‡ Ø§Ù„+Ù…Ø¹Ù„ÙˆÙ…+Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚+Ù‡Ø§ Ù+Ø§Ù„+ÙØªØ±Ù‡ ØªØ³...</td>\n",
       "      <td>['Ø³ÙŠØ·Ø±Ù‡', 'Ù…Ø¹Ù„ÙˆÙ…Ø©', 'ØªØºÙŠØ±', 'Ø·Ø±ÙŠÙ‚', 'ÙØªØ±Ù‡', 'Ø³...</td>\n",
       "      <td>['S/S', 'Ø§Ù„+', 'Ø³ÙŠØ·Ø±Ù‡/DET+NOUN-MS', 'Ø§Ù„+', 'Ù…Ø¹...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label      source                                               text  \\\n",
       "0      1  translated  Ø´Ù‰ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹Ø§ ØªØ¬Ø±Ø¨...   \n",
       "1      0     youtube                      Ù„ÙƒØ°Ø¨ ÙÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¯Ù… Ø¨Ø±ÙƒØ§Ùˆ Ù…Ù† Ù„ÙƒØ°Ø¨   \n",
       "2      0     youtube  Ø§Ù„Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ùˆ Ù„Ø§ Ø­...   \n",
       "3      0  translated  Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‡Ø¬Ù…Ø§Øª Ø¹Ù„Ù‰ ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠØ© Ø³Ø§Ø­Ù„...   \n",
       "4      0  translated  Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚Ù‡Ø§ ÙØ§Ù„ÙØªØ±Ø© Ø§Ù„ÙŠ ...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  Ø´Ù‰ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹Ø§ ØªØ¬Ø±Ø¨...   \n",
       "1                         Ù„ÙƒØ°Ø¨ ÙÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¯Ù… Ø¨Ø±ÙƒØ§Ùˆ Ù„ÙƒØ°Ø¨   \n",
       "2  Ø§Ù„Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ø­ÙˆÙ„Ø§ Ù‚...   \n",
       "3   Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‡Ø¬Ù…Ø§Øª ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠØ© Ø³Ø§Ø­Ù„ Ø§Ù„Ø¹Ø§Ø¬   \n",
       "4  Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚Ù‡Ø§ ÙØ§Ù„ÙØªØ±Ø© ØªØ³Ø¨Ù‚ Ø§Ù†Øª...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  Ø´ÙŠ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹Ø§ ØªØ¬Ø±Ø¨...   \n",
       "1                         Ù„ÙƒØ°Ø¨ ÙÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¯Ù… Ø¨Ø±ÙƒØ§Ùˆ Ù„ÙƒØ°Ø¨   \n",
       "2  Ø§Ù„Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ø­ÙˆÙ„Ø§ Ù‚...   \n",
       "3   Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‡Ø¬Ù…Ø§Øª ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠÙ‡ Ø³Ø§Ø­Ù„ Ø§Ù„Ø¹Ø§Ø¬   \n",
       "4  Ø§Ù„Ø³ÙŠØ·Ø±Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚Ù‡Ø§ ÙØ§Ù„ÙØªØ±Ù‡ ØªØ³Ø¨Ù‚ Ø§Ù†Øª...   \n",
       "\n",
       "                                      segmented_text  \\\n",
       "0  Ø´ÙŠ Ø¨ÙˆØªÙŠÙ† ÙŠÙˆØ§ÙÙ‚ Ø¹Ù„Ø§ Ø§Ù„+ØªØ¹Ø§Ù…Ù„ Ø¨+Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹+Ø§ Øª...   \n",
       "1                    Ù„+ÙƒØ°Ø¨ ÙÙˆØ¬+Ù‡ Ø¨+Ù†Ø§Ø¯Ù… Ø¨+Ø±ÙƒØ§Ùˆ Ù„+ÙƒØ°Ø¨   \n",
       "2  Ø§Ù„+Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„+Ø¹Ø§Ù„Ù…+ÙŠÙ† Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ù…Ø´Ø§Ø¡Ø§Ù„Ù„Ù‡ Ø­ÙˆÙ„...   \n",
       "3  Ø­Ù„ÙØ§Ø¡ Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ ÙˆØ±Ø§Ø¡ Ø§Ù„+Ù‡Ø¬Ù…+Ø§Øª ÙˆØ²ÙŠØ± Ø¯Ø§Ø®Ù„ÙŠ+Ù‡ Ø³Ø§Ø­Ù„ ...   \n",
       "4  Ø§Ù„+Ø³ÙŠØ·Ø±Ù‡ Ø§Ù„+Ù…Ø¹Ù„ÙˆÙ…+Ø§Øª ØªØºÙŠØ± Ø·Ø±ÙŠÙ‚+Ù‡Ø§ Ù+Ø§Ù„+ÙØªØ±Ù‡ ØªØ³...   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  ['Ø´ÙŠ', 'Ø¨ÙˆØªÙŠÙ†', 'ÙˆØ§ÙÙ‚', 'Ø¹Ù„Ø§', 'ØªØ¹Ø§Ù…Ù„', 'Ø´ÙƒÙ„',...   \n",
       "1              ['ÙƒØ°Ø¨', 'ÙÙˆØ¬', 'Ù†Ø§Ø¯Ù…', 'Ø±ÙƒØ§Ùˆ', 'ÙƒØ°Ø¨']   \n",
       "2  ['Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡', 'Ø±Ø¨', 'Ø¹Ø§Ù„Ù…', 'Ø¨Ø³Ù…', 'Ø§Ù„Ù„Ù‡', 'Ù…Ø´Ø§Ø¡...   \n",
       "3  ['Ø­Ù„ÙŠÙ', 'Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ', 'ÙˆØ±Ø§Ø¡', 'Ù‡Ø¬Ù…Ø©', 'ÙˆØ²ÙŠØ±', 'Ø¯Ø§...   \n",
       "4  ['Ø³ÙŠØ·Ø±Ù‡', 'Ù…Ø¹Ù„ÙˆÙ…Ø©', 'ØªØºÙŠØ±', 'Ø·Ø±ÙŠÙ‚', 'ÙØªØ±Ù‡', 'Ø³...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  ['S/S', 'Ø´ÙŠ/V', 'Ø¨ÙˆØªÙŠÙ†/NOUN-MS', 'ÙŠÙˆØ§ÙÙ‚/V', 'Ø¹...  \n",
       "1  ['S/S', 'Ù„+/PREP', 'ÙƒØ°Ø¨/NOUN-MS', 'ÙÙˆØ¬/NOUN-MS...  \n",
       "2  ['S/S', 'Ø§Ù„+', 'Ø­Ù…Ø¯Ø§Ù„Ù„Ù‡/DET+NOUN-MS', 'Ø±Ø¨/NOUN...  \n",
       "3  ['S/S', 'Ø­Ù„ÙØ§Ø¡/NOUN-MP', 'Ø¬Ø¨Ø§Ø¬Ø¨Ùˆ/NOUN-MS', 'ÙˆØ±...  \n",
       "4  ['S/S', 'Ø§Ù„+', 'Ø³ÙŠØ·Ø±Ù‡/DET+NOUN-MS', 'Ø§Ù„+', 'Ù…Ø¹...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv('./clean_dataset_v2.csv', encoding='utf-8')\n",
    "df_2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
