{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "tqdm.pandas()\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "stemmer = FarasaStemmer()\n",
    "ner = FarasaNamedEntityRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the path for the data set here\n",
    "path = '../data_set/train_set.csv'\n",
    "path_2 = '../data_set/algerian_dialect_news.csv'\n",
    "df = pd.read_csv(path, encoding='utf-8', skiprows=range(1, 3001))\n",
    "df = df.dropna(subset=['text'])  # Drop rows where 'text' is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5069, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>youtube</td>\n",
       "      <td>ØªÙˆÙÙ‰ ÙˆÙ„Ø§ Ù„Ø¨Ø§Ø±Ø­ Ø¨ØµØ­ ØºÙŠØ± Ø§Ù„Ø®Ø¨Ø± Ù…Ø²Ø§Ù„ Ù…Ø§ØªÙ†Ø´Ø±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>paraphrased</td>\n",
       "      <td>Ù…ØªØ­Ø¯ÙŠÙ‹Ø§ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø³ÙƒØ§Ù† Ù„ÙŠÙ† ÙŠØ±ÙØ¶ÙˆÙ† Ù…ØºØ§Ø¯Ø±Ø© Ø§Ù„Ù…Ø¨Ø§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>ÙˆÙ‚Ø§Ù„Øª Ø§Ù„ØµÙŠÙ† Ø¨Ù„ÙŠ Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Ø³ÙŠØ© Ù„Ø§Ø²Ù…Ø§ Ø¨Ø§Ø´  Ø´Ø¨Ù‡ Ø§Ù„Ø¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>manual</td>\n",
       "      <td>Ù„Ø§Ù‚ØªØµØ§Ø¯ ØªØ¹ Ø§Ù„ØµÙŠÙ† Ø¨Ø¯Ø§ ÙŠÙƒØ¨Ø± ÙÙ„Ù‚Ø±Ù† Ù„Ø¹Ø´Ø±ÙŠÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>Ø§Ù„ØµÙŠÙ† Ø¹Ø·Ø§Øª  Ù…Ù„ÙŠØ§Ø± Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ø£ÙØ±ÙŠÙ‚ÙŠØ§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label       source                                               text\n",
       "0      1      youtube          ØªÙˆÙÙ‰ ÙˆÙ„Ø§ Ù„Ø¨Ø§Ø±Ø­ Ø¨ØµØ­ ØºÙŠØ± Ø§Ù„Ø®Ø¨Ø± Ù…Ø²Ø§Ù„ Ù…Ø§ØªÙ†Ø´Ø± \n",
       "1      1  paraphrased  Ù…ØªØ­Ø¯ÙŠÙ‹Ø§ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ø³ÙƒØ§Ù† Ù„ÙŠÙ† ÙŠØ±ÙØ¶ÙˆÙ† Ù…ØºØ§Ø¯Ø±Ø© Ø§Ù„Ù…Ø¨Ø§...\n",
       "2      0   translated  ÙˆÙ‚Ø§Ù„Øª Ø§Ù„ØµÙŠÙ† Ø¨Ù„ÙŠ Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Ø³ÙŠØ© Ù„Ø§Ø²Ù…Ø§ Ø¨Ø§Ø´  Ø´Ø¨Ù‡ Ø§Ù„Ø¬...\n",
       "3      1       manual            Ù„Ø§Ù‚ØªØµØ§Ø¯ ØªØ¹ Ø§Ù„ØµÙŠÙ† Ø¨Ø¯Ø§ ÙŠÙƒØ¨Ø± ÙÙ„Ù‚Ø±Ù† Ù„Ø¹Ø´Ø±ÙŠÙ† \n",
       "4      0   translated   Ø§Ù„ØµÙŠÙ† Ø¹Ø·Ø§Øª  Ù…Ù„ÙŠØ§Ø± Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø¹Ø³ÙƒØ±ÙŠØ© Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ø£ÙØ±ÙŠÙ‚ÙŠØ§"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'source', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null rows: 0\n",
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count null rows\n",
    "null_rows_count = df.isnull().sum().sum()\n",
    "print(f\"Number of null rows: {null_rows_count}\")\n",
    "\n",
    "# Count duplicated rows\n",
    "duplicated_rows_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicated rows: {duplicated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ›‘ Removing Stopwords: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [00:00<00:00, 234282.77it/s]\n",
      "ğŸ”¤ Normalizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [00:00<00:00, 77290.82it/s]\n",
      "ğŸ“Œ Segmenting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [4:00:17<00:00,  2.84s/it]  \n",
      "ğŸŒ± Stemming: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [3:59:39<00:00,  2.84s/it]  \n",
      "ğŸ“Œ POS Tagging: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5069/5069 [13:08:05<00:00,  9.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLP Preprocessing Complete! Ready for AraBERT tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Arabic Text Normalization\n",
    "def normalize_arabic(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                   # Remove HTML tags\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s,]\", \"\", text, flags=re.UNICODE)  # Remove emojis\n",
    "    # Normalize Arabic text\n",
    "    text = re.sub(r\"[Ø¥Ø£Ø¢Ù±]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"[ÙÙÙÙ‘Ù’ÙÙŒÙ‹]\", \"\", text)  # Remove diacritics\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # Remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Load custom Arabic stopwords\n",
    "with open(\"../data_set/algerian_arabic_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(word.strip() for word in f.readlines())\n",
    "# -------------------------\n",
    "# Text Segmentation (Sentence Splitting)\n",
    "def segment_text(text):\n",
    "    # Farasa returns segmented text with morphological boundaries marked\n",
    "    segmented = segmenter.segment(text)\n",
    "    # Return as is or with your custom separator\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization (Root Extraction)\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join(stemmer.stem(text))\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging\n",
    "def pos_tag_text(text):\n",
    "    return \" \".join(pos_tagger.tag(text))\n",
    "\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# Stopword Removal\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
    "\n",
    "# -------------------------\n",
    "# Text Segmentation using Farasa\n",
    "def segment_text(text):\n",
    "    segmented = segmenter.segment(text)\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization using Farasa\n",
    "def lemmatize_text(text):\n",
    "    stemmed = stemmer.stem(text)\n",
    "    return stemmed\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging using Farasa\n",
    "def pos_tag_text(text):\n",
    "    tagged = pos_tagger.tag(text)\n",
    "    return tagged\n",
    "# -----------------------\n",
    "\n",
    "# Apply stopword removal\n",
    "tqdm.pandas(desc=\"ğŸ›‘ Removing Stopwords\")\n",
    "df[\"cleaned_text\"] = df[\"text\"].progress_apply(remove_stopwords)\n",
    "\n",
    "# Apply normalization\n",
    "tqdm.pandas(desc=\"ğŸ”¤ Normalizing\")\n",
    "df[\"normalized_text\"] = df[\"cleaned_text\"].progress_apply(normalize_arabic)\n",
    "\n",
    "# Apply segmentation\n",
    "tqdm.pandas(desc=\"ğŸ“Œ Segmenting\")\n",
    "df[\"segmented_text\"] = df[\"normalized_text\"].progress_apply(segment_text)\n",
    "\n",
    "# Apply stemming/lemmatization and convert to token list\n",
    "tqdm.pandas(desc=\"ğŸŒ± Stemming\")\n",
    "df[\"stemmed_tokens\"] = df[\"normalized_text\"].progress_apply(lambda text: stemmer.stem(text))\n",
    "df[\"stemmed_tokens\"] = df[\"stemmed_tokens\"].apply(lambda text: text.split())\n",
    "\n",
    "# Apply POS tagging and convert to token list\n",
    "tqdm.pandas(desc=\"ğŸ“Œ POS Tagging\")\n",
    "df[\"pos_tags\"] = df[\"normalized_text\"].progress_apply(lambda text: pos_tagger.tag(text))\n",
    "df[\"pos_tags\"] = df[\"pos_tags\"].apply(lambda text: text.split())\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ… NLP Preprocessing Complete! Ready for AraBERT tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "import re\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "stemmer = FarasaStemmer()\n",
    "ner = FarasaNamedEntityRecognizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                   # Remove HTML tags\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s,]\", \"\", text, flags=re.UNICODE)  # Remove emojis\n",
    "    # Normalize Arabic text\n",
    "    text = re.sub(r\"[Ø¥Ø£Ø¢Ù±]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ù‡\", \"Ø©\", text)\n",
    "    text = re.sub(\"[ÙÙÙÙ‘Ù’ÙÙŒÙ‹]\", \"\", text)  # Remove diacritics\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # Remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "def segment_text(text):\n",
    "    segmented = segmenter.segment(text)\n",
    "    return segmented\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    stemmed = stemmer.stem(text)\n",
    "    return stemmed\n",
    "\n",
    "with open(\"../data_set/algerian_arabic_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(word.strip() for word in f.readlines())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word not in custom_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ ÙŠÙ‚ÙˆØ¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ù„Ù„ÙÙˆØ² Ø§Ù„ÙƒØ¨ÙŠØ± ğŸ† Ø¹Ù„Ù‰ Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\n",
      "Normalized Text: Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ ÙŠÙ‚ÙˆØ¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ù„Ù„ÙÙˆØ² Ø§Ù„ÙƒØ¨ÙŠØ±  Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\n",
      "Stemmed Text: Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ Ù‚Ø§Ø¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ ÙÙˆØ² ÙƒØ¨ÙŠØ± Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\n",
      "POS Tags (Split):\n",
      "S/S\n",
      "Ù…Ø­Ù…Ø¯/NOUN-MS\n",
      "ØµÙ„Ø§Ø­/NOUN-MS\n",
      "Ù‚Ø§Ø¯/V\n",
      "Ù†Ø§Ø¯ÙŠ/NOUN-MS\n",
      "Ù„ÙŠÙØ±Ø¨ÙˆÙ„/NOUN-MS\n",
      "ÙÙˆØ²/NOUN-MS\n",
      "ÙƒØ¨ÙŠØ±/ADJ-MS\n",
      "Ù…Ø§Ù†Ø´Ø³ØªØ±/NOUN-MS\n",
      "ÙŠÙˆÙ†Ø§ÙŠØªØ¯/NOUN-MS\n",
      "E/E\n"
     ]
    }
   ],
   "source": [
    "text =\"Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ ÙŠÙ‚ÙˆØ¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ù„Ù„ÙÙˆØ² Ø§Ù„ÙƒØ¨ÙŠØ± ğŸ† Ø¹Ù„Ù‰ Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\"\n",
    "\n",
    "#Ù„ÙŠÙØ±Ø¨ÙˆÙ„ #Ù…Ø­Ù…Ø¯_ØµÙ„Ø§Ø­ #Ø§Ù„Ø¯ÙˆØ±ÙŠ_Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ\"\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "\n",
    "# Remove stopwords from the text\n",
    "text_without_stopwords = remove_stopwords(text)\n",
    "\n",
    "# Normalize the text after removing stopwords\n",
    "normalized_text = normalize_arabic(text_without_stopwords)\n",
    "print(\"Normalized Text:\", normalized_text)\n",
    "\n",
    "stemmed_text = stemmer.stem(normalized_text)\n",
    "print(\"Stemmed Text:\", stemmed_text)\n",
    "\n",
    "# Now POS tagging on stemmed text\n",
    "pos_tags_split = pos_tagger.tag(stemmed_text).split()\n",
    "print(\"POS Tags (Split):\")\n",
    "for tag in pos_tags_split:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ ÙŠÙ‚ÙˆØ¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ù„Ù„ÙÙˆØ² Ø§Ù„ÙƒØ¨ÙŠØ± ğŸ† Ø¹Ù„Ù‰ Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\n",
      "Normalized Text: Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ ÙŠÙ‚ÙˆØ¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ù„Ù„ÙÙˆØ² Ø§Ù„ÙƒØ¨ÙŠØ±  Ø¹Ù„ÙŠ Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\n",
      "Stemmed Text: Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ Ù‚Ø§Ø¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ ÙÙˆØ² ÙƒØ¨ÙŠØ± Ø¹Ù„ÙŠ Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\n",
      "\n",
      "Normalized Text as Words: ['Ù…Ø­Ù…Ø¯', 'ØµÙ„Ø§Ø­', 'ÙŠÙ‚ÙˆØ¯', 'Ù†Ø§Ø¯ÙŠ', 'Ù„ÙŠÙØ±Ø¨ÙˆÙ„', 'Ù„Ù„ÙÙˆØ²', 'Ø§Ù„ÙƒØ¨ÙŠØ±', 'Ø¹Ù„ÙŠ', 'Ù…Ø§Ù†Ø´Ø³ØªØ±', 'ÙŠÙˆÙ†Ø§ÙŠØªØ¯']\n",
      "\n",
      "After processing: ['Ù…Ø­Ù…Ø¯', 'ØµÙ„Ø§Ø­', 'Ù‚Ø§Ø¯', 'Ù†Ø§Ø¯ÙŠ', 'Ù„ÙŠÙØ±Ø¨ÙˆÙ„', 'ÙÙˆØ²', 'ÙƒØ¨ÙŠØ±', 'Ø¹Ù„ÙŠ', 'Ù…Ø§Ù†Ø´Ø³ØªØ±', 'ÙŠÙˆÙ†Ø§ÙŠØªØ¯']\n"
     ]
    }
   ],
   "source": [
    "# Your input text\n",
    "text = \"Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ ÙŠÙ‚ÙˆØ¯ Ù†Ø§Ø¯ÙŠ Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ù„Ù„ÙÙˆØ² Ø§Ù„ÙƒØ¨ÙŠØ± ğŸ† Ø¹Ù„Ù‰ Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯\"\n",
    "\n",
    "# Print the original text\n",
    "print(\"Original Text:\", text)\n",
    "\n",
    "# Normalize the text\n",
    "normalized_text = normalize_arabic(text)\n",
    "print(\"Normalized Text:\", normalized_text)\n",
    "\n",
    "# Stem the normalized text\n",
    "stemmed_text = stemmer.stem(normalized_text)\n",
    "print(\"Stemmed Text:\", stemmed_text)\n",
    "\n",
    "# Function to split text into words\n",
    "def split_text_to_words(text):\n",
    "    # Split the text by spaces or punctuation\n",
    "    return [word for word in text.split() if word not in ['.', ',', '!', '?', 'â€¦', '(', ')']]\n",
    "\n",
    "# Split normalized and stemmed text into words\n",
    "normalized_words = split_text_to_words(normalized_text)\n",
    "stemmed_words = split_text_to_words(stemmed_text)\n",
    "\n",
    "# Output the word lists\n",
    "print(\"\\nNormalized Text as Words:\", normalized_words)\n",
    "print(\"\\nAfter processing:\", stemmed_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
