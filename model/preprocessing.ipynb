{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "tqdm.pandas()\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "stemmer = FarasaStemmer()\n",
    "ner = FarasaNamedEntityRecognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the path for the data set here\n",
    "path = '../data_set/train_set.csv'\n",
    "path_2 = '../data_set/algerian_dialect_news.csv'\n",
    "df = pd.read_csv(path, encoding='utf-8', skiprows=range(1, 3001))\n",
    "df = df.dropna(subset=['text'])  # Drop rows where 'text' is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5069, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>youtube</td>\n",
       "      <td>توفى ولا لبارح بصح غير الخبر مزال ماتنشر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>paraphrased</td>\n",
       "      <td>متحديًا تحذيرات السكان لين يرفضون مغادرة المبا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>وقالت الصين بلي الدبلوماسية لازما باش  شبه الج...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>manual</td>\n",
       "      <td>لاقتصاد تع الصين بدا يكبر فلقرن لعشرين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>الصين عطات  مليار مساعدات عسكرية مجانية لأفريقيا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label       source                                               text\n",
       "0      1      youtube          توفى ولا لبارح بصح غير الخبر مزال ماتنشر \n",
       "1      1  paraphrased  متحديًا تحذيرات السكان لين يرفضون مغادرة المبا...\n",
       "2      0   translated  وقالت الصين بلي الدبلوماسية لازما باش  شبه الج...\n",
       "3      1       manual            لاقتصاد تع الصين بدا يكبر فلقرن لعشرين \n",
       "4      0   translated   الصين عطات  مليار مساعدات عسكرية مجانية لأفريقيا"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'source', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null rows: 0\n",
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count null rows\n",
    "null_rows_count = df.isnull().sum().sum()\n",
    "print(f\"Number of null rows: {null_rows_count}\")\n",
    "\n",
    "# Count duplicated rows\n",
    "duplicated_rows_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicated rows: {duplicated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🛑 Removing Stopwords: 100%|██████████| 5069/5069 [00:00<00:00, 234282.77it/s]\n",
      "🔤 Normalizing: 100%|██████████| 5069/5069 [00:00<00:00, 77290.82it/s]\n",
      "📌 Segmenting: 100%|██████████| 5069/5069 [4:00:17<00:00,  2.84s/it]  \n",
      "🌱 Stemming: 100%|██████████| 5069/5069 [3:59:39<00:00,  2.84s/it]  \n",
      "📌 POS Tagging: 100%|██████████| 5069/5069 [13:08:05<00:00,  9.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NLP Preprocessing Complete! Ready for AraBERT tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Arabic Text Normalization\n",
    "def normalize_arabic(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                   # Remove HTML tags\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s,]\", \"\", text, flags=re.UNICODE)  # Remove emojis\n",
    "    # Normalize Arabic text\n",
    "    text = re.sub(r\"[إأآٱ]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"[ًٌٍَُِّْ]\", \"\", text)  # Remove diacritics\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # Remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Load custom Arabic stopwords\n",
    "with open(\"../data_set/algerian_arabic_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(word.strip() for word in f.readlines())\n",
    "# -------------------------\n",
    "# Text Segmentation (Sentence Splitting)\n",
    "def segment_text(text):\n",
    "    # Farasa returns segmented text with morphological boundaries marked\n",
    "    segmented = segmenter.segment(text)\n",
    "    # Return as is or with your custom separator\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization (Root Extraction)\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join(stemmer.stem(text))\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging\n",
    "def pos_tag_text(text):\n",
    "    return \" \".join(pos_tagger.tag(text))\n",
    "\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# Stopword Removal\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
    "\n",
    "# -------------------------\n",
    "# Text Segmentation using Farasa\n",
    "def segment_text(text):\n",
    "    segmented = segmenter.segment(text)\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization using Farasa\n",
    "def lemmatize_text(text):\n",
    "    stemmed = stemmer.stem(text)\n",
    "    return stemmed\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging using Farasa\n",
    "def pos_tag_text(text):\n",
    "    tagged = pos_tagger.tag(text)\n",
    "    return tagged\n",
    "# -----------------------\n",
    "\n",
    "# Apply stopword removal\n",
    "tqdm.pandas(desc=\"🛑 Removing Stopwords\")\n",
    "df[\"cleaned_text\"] = df[\"text\"].progress_apply(remove_stopwords)\n",
    "\n",
    "# Apply normalization\n",
    "tqdm.pandas(desc=\"🔤 Normalizing\")\n",
    "df[\"normalized_text\"] = df[\"cleaned_text\"].progress_apply(normalize_arabic)\n",
    "\n",
    "# Apply segmentation\n",
    "tqdm.pandas(desc=\"📌 Segmenting\")\n",
    "df[\"segmented_text\"] = df[\"normalized_text\"].progress_apply(segment_text)\n",
    "\n",
    "# Apply stemming/lemmatization and convert to token list\n",
    "tqdm.pandas(desc=\"🌱 Stemming\")\n",
    "df[\"stemmed_tokens\"] = df[\"normalized_text\"].progress_apply(lambda text: stemmer.stem(text))\n",
    "df[\"stemmed_tokens\"] = df[\"stemmed_tokens\"].apply(lambda text: text.split())\n",
    "\n",
    "# Apply POS tagging and convert to token list\n",
    "tqdm.pandas(desc=\"📌 POS Tagging\")\n",
    "df[\"pos_tags\"] = df[\"normalized_text\"].progress_apply(lambda text: pos_tagger.tag(text))\n",
    "df[\"pos_tags\"] = df[\"pos_tags\"].apply(lambda text: text.split())\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ NLP Preprocessing Complete! Ready for AraBERT tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>segmented_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>translated</td>\n",
       "      <td>شى بوتين يوافق علا التعامل بشكل مناسب معا تجرب...</td>\n",
       "      <td>شى بوتين يوافق علا التعامل بشكل مناسب معا تجرب...</td>\n",
       "      <td>شي بوتين يوافق علا التعامل بشكل مناسب معا تجرب...</td>\n",
       "      <td>شي بوتين يوافق علا ال+تعامل ب+شكل مناسب مع+ا ت...</td>\n",
       "      <td>['شي', 'بوتين', 'وافق', 'علا', 'تعامل', 'شكل',...</td>\n",
       "      <td>['S/S', 'شي/V', 'بوتين/NOUN-MS', 'يوافق/V', 'ع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>youtube</td>\n",
       "      <td>لكذب فوجه بنادم بركاو من لكذب</td>\n",
       "      <td>لكذب فوجه بنادم بركاو لكذب</td>\n",
       "      <td>لكذب فوجه بنادم بركاو لكذب</td>\n",
       "      <td>ل+كذب فوج+ه ب+نادم ب+ركاو ل+كذب</td>\n",
       "      <td>['كذب', 'فوج', 'نادم', 'ركاو', 'كذب']</td>\n",
       "      <td>['S/S', 'ل+/PREP', 'كذب/NOUN-MS', 'فوج/NOUN-MS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>youtube</td>\n",
       "      <td>الحمدالله رب العالمين بسم الله مشاءالله و لا ح...</td>\n",
       "      <td>الحمدالله رب العالمين بسم الله مشاءالله حولا ق...</td>\n",
       "      <td>الحمدالله رب العالمين بسم الله مشاءالله حولا ق...</td>\n",
       "      <td>ال+حمدالله رب ال+عالم+ين بسم الله مشاءالله حول...</td>\n",
       "      <td>['حمدالله', 'رب', 'عالم', 'بسم', 'الله', 'مشاء...</td>\n",
       "      <td>['S/S', 'ال+', 'حمدالله/DET+NOUN-MS', 'رب/NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>حلفاء جباجبو وراء الهجمات على وزير داخلية ساحل...</td>\n",
       "      <td>حلفاء جباجبو وراء الهجمات وزير داخلية ساحل العاج</td>\n",
       "      <td>حلفاء جباجبو وراء الهجمات وزير داخليه ساحل العاج</td>\n",
       "      <td>حلفاء جباجبو وراء ال+هجم+ات وزير داخلي+ه ساحل ...</td>\n",
       "      <td>['حليف', 'جباجبو', 'وراء', 'هجمة', 'وزير', 'دا...</td>\n",
       "      <td>['S/S', 'حلفاء/NOUN-MP', 'جباجبو/NOUN-MS', 'ور...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>السيطرة على المعلومات تغير طريقها فالفترة الي ...</td>\n",
       "      <td>السيطرة المعلومات تغير طريقها فالفترة تسبق انت...</td>\n",
       "      <td>السيطره المعلومات تغير طريقها فالفتره تسبق انت...</td>\n",
       "      <td>ال+سيطره ال+معلوم+ات تغير طريق+ها ف+ال+فتره تس...</td>\n",
       "      <td>['سيطره', 'معلومة', 'تغير', 'طريق', 'فتره', 'س...</td>\n",
       "      <td>['S/S', 'ال+', 'سيطره/DET+NOUN-MS', 'ال+', 'مع...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label      source                                               text  \\\n",
       "0      1  translated  شى بوتين يوافق علا التعامل بشكل مناسب معا تجرب...   \n",
       "1      0     youtube                      لكذب فوجه بنادم بركاو من لكذب   \n",
       "2      0     youtube  الحمدالله رب العالمين بسم الله مشاءالله و لا ح...   \n",
       "3      0  translated  حلفاء جباجبو وراء الهجمات على وزير داخلية ساحل...   \n",
       "4      0  translated  السيطرة على المعلومات تغير طريقها فالفترة الي ...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  شى بوتين يوافق علا التعامل بشكل مناسب معا تجرب...   \n",
       "1                         لكذب فوجه بنادم بركاو لكذب   \n",
       "2  الحمدالله رب العالمين بسم الله مشاءالله حولا ق...   \n",
       "3   حلفاء جباجبو وراء الهجمات وزير داخلية ساحل العاج   \n",
       "4  السيطرة المعلومات تغير طريقها فالفترة تسبق انت...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  شي بوتين يوافق علا التعامل بشكل مناسب معا تجرب...   \n",
       "1                         لكذب فوجه بنادم بركاو لكذب   \n",
       "2  الحمدالله رب العالمين بسم الله مشاءالله حولا ق...   \n",
       "3   حلفاء جباجبو وراء الهجمات وزير داخليه ساحل العاج   \n",
       "4  السيطره المعلومات تغير طريقها فالفتره تسبق انت...   \n",
       "\n",
       "                                      segmented_text  \\\n",
       "0  شي بوتين يوافق علا ال+تعامل ب+شكل مناسب مع+ا ت...   \n",
       "1                    ل+كذب فوج+ه ب+نادم ب+ركاو ل+كذب   \n",
       "2  ال+حمدالله رب ال+عالم+ين بسم الله مشاءالله حول...   \n",
       "3  حلفاء جباجبو وراء ال+هجم+ات وزير داخلي+ه ساحل ...   \n",
       "4  ال+سيطره ال+معلوم+ات تغير طريق+ها ف+ال+فتره تس...   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  ['شي', 'بوتين', 'وافق', 'علا', 'تعامل', 'شكل',...   \n",
       "1              ['كذب', 'فوج', 'نادم', 'ركاو', 'كذب']   \n",
       "2  ['حمدالله', 'رب', 'عالم', 'بسم', 'الله', 'مشاء...   \n",
       "3  ['حليف', 'جباجبو', 'وراء', 'هجمة', 'وزير', 'دا...   \n",
       "4  ['سيطره', 'معلومة', 'تغير', 'طريق', 'فتره', 'س...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  ['S/S', 'شي/V', 'بوتين/NOUN-MS', 'يوافق/V', 'ع...  \n",
       "1  ['S/S', 'ل+/PREP', 'كذب/NOUN-MS', 'فوج/NOUN-MS...  \n",
       "2  ['S/S', 'ال+', 'حمدالله/DET+NOUN-MS', 'رب/NOUN...  \n",
       "3  ['S/S', 'حلفاء/NOUN-MP', 'جباجبو/NOUN-MS', 'ور...  \n",
       "4  ['S/S', 'ال+', 'سيطره/DET+NOUN-MS', 'ال+', 'مع...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv('./clean_dataset_v2.csv', encoding='utf-8')\n",
    "df_2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
