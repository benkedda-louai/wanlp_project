{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "tqdm.pandas()\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "stemmer = FarasaStemmer()\n",
    "ner = FarasaNamedEntityRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the path for the data set here\n",
    "path = '../data_set/train_set.csv'\n",
    "path_2 = '../data_set/algerian_dialect_news.csv'\n",
    "df = pd.read_csv(path, encoding='utf-8', skiprows=range(1, 3001))\n",
    "df = df.dropna(subset=['text'])  # Drop rows where 'text' is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5069, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>youtube</td>\n",
       "      <td>توفى ولا لبارح بصح غير الخبر مزال ماتنشر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>paraphrased</td>\n",
       "      <td>متحديًا تحذيرات السكان لين يرفضون مغادرة المبا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>وقالت الصين بلي الدبلوماسية لازما باش  شبه الج...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>manual</td>\n",
       "      <td>لاقتصاد تع الصين بدا يكبر فلقرن لعشرين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>translated</td>\n",
       "      <td>الصين عطات  مليار مساعدات عسكرية مجانية لأفريقيا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label       source                                               text\n",
       "0      1      youtube          توفى ولا لبارح بصح غير الخبر مزال ماتنشر \n",
       "1      1  paraphrased  متحديًا تحذيرات السكان لين يرفضون مغادرة المبا...\n",
       "2      0   translated  وقالت الصين بلي الدبلوماسية لازما باش  شبه الج...\n",
       "3      1       manual            لاقتصاد تع الصين بدا يكبر فلقرن لعشرين \n",
       "4      0   translated   الصين عطات  مليار مساعدات عسكرية مجانية لأفريقيا"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'source', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null rows: 0\n",
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count null rows\n",
    "null_rows_count = df.isnull().sum().sum()\n",
    "print(f\"Number of null rows: {null_rows_count}\")\n",
    "\n",
    "# Count duplicated rows\n",
    "duplicated_rows_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicated rows: {duplicated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🛑 Removing Stopwords: 100%|██████████| 5069/5069 [00:00<00:00, 234282.77it/s]\n",
      "🔤 Normalizing: 100%|██████████| 5069/5069 [00:00<00:00, 77290.82it/s]\n",
      "📌 Segmenting: 100%|██████████| 5069/5069 [4:00:17<00:00,  2.84s/it]  \n",
      "🌱 Stemming: 100%|██████████| 5069/5069 [3:59:39<00:00,  2.84s/it]  \n",
      "📌 POS Tagging: 100%|██████████| 5069/5069 [13:08:05<00:00,  9.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NLP Preprocessing Complete! Ready for AraBERT tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Arabic Text Normalization\n",
    "def normalize_arabic(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                   # Remove HTML tags\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s,]\", \"\", text, flags=re.UNICODE)  # Remove emojis\n",
    "    # Normalize Arabic text\n",
    "    text = re.sub(r\"[إأآٱ]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"[ًٌٍَُِّْ]\", \"\", text)  # Remove diacritics\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # Remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Load custom Arabic stopwords\n",
    "with open(\"../data_set/algerian_arabic_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(word.strip() for word in f.readlines())\n",
    "# -------------------------\n",
    "# Text Segmentation (Sentence Splitting)\n",
    "def segment_text(text):\n",
    "    # Farasa returns segmented text with morphological boundaries marked\n",
    "    segmented = segmenter.segment(text)\n",
    "    # Return as is or with your custom separator\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization (Root Extraction)\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join(stemmer.stem(text))\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging\n",
    "def pos_tag_text(text):\n",
    "    return \" \".join(pos_tagger.tag(text))\n",
    "\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# Stopword Removal\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word not in custom_stopwords])\n",
    "\n",
    "# -------------------------\n",
    "# Text Segmentation using Farasa\n",
    "def segment_text(text):\n",
    "    segmented = segmenter.segment(text)\n",
    "    return segmented\n",
    "\n",
    "# -------------------------\n",
    "# Lemmatization using Farasa\n",
    "def lemmatize_text(text):\n",
    "    stemmed = stemmer.stem(text)\n",
    "    return stemmed\n",
    "\n",
    "# -------------------------\n",
    "# POS Tagging using Farasa\n",
    "def pos_tag_text(text):\n",
    "    tagged = pos_tagger.tag(text)\n",
    "    return tagged\n",
    "# -----------------------\n",
    "\n",
    "# Apply stopword removal\n",
    "tqdm.pandas(desc=\"🛑 Removing Stopwords\")\n",
    "df[\"cleaned_text\"] = df[\"text\"].progress_apply(remove_stopwords)\n",
    "\n",
    "# Apply normalization\n",
    "tqdm.pandas(desc=\"🔤 Normalizing\")\n",
    "df[\"normalized_text\"] = df[\"cleaned_text\"].progress_apply(normalize_arabic)\n",
    "\n",
    "# Apply segmentation\n",
    "tqdm.pandas(desc=\"📌 Segmenting\")\n",
    "df[\"segmented_text\"] = df[\"normalized_text\"].progress_apply(segment_text)\n",
    "\n",
    "# Apply stemming/lemmatization and convert to token list\n",
    "tqdm.pandas(desc=\"🌱 Stemming\")\n",
    "df[\"stemmed_tokens\"] = df[\"normalized_text\"].progress_apply(lambda text: stemmer.stem(text))\n",
    "df[\"stemmed_tokens\"] = df[\"stemmed_tokens\"].apply(lambda text: text.split())\n",
    "\n",
    "# Apply POS tagging and convert to token list\n",
    "tqdm.pandas(desc=\"📌 POS Tagging\")\n",
    "df[\"pos_tags\"] = df[\"normalized_text\"].progress_apply(lambda text: pos_tagger.tag(text))\n",
    "df[\"pos_tags\"] = df[\"pos_tags\"].apply(lambda text: text.split())\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ NLP Preprocessing Complete! Ready for AraBERT tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "import re\n",
    "# Initialize Farasa tools\n",
    "segmenter = FarasaSegmenter()\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "stemmer = FarasaStemmer()\n",
    "ner = FarasaNamedEntityRecognizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                   # Remove HTML tags\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s,]\", \"\", text, flags=re.UNICODE)  # Remove emojis\n",
    "    # Normalize Arabic text\n",
    "    text = re.sub(r\"[إأآٱ]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ه\", \"ة\", text)\n",
    "    text = re.sub(\"[ًٌٍَُِّْ]\", \"\", text)  # Remove diacritics\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)   # Remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "def segment_text(text):\n",
    "    segmented = segmenter.segment(text)\n",
    "    return segmented\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    stemmed = stemmer.stem(text)\n",
    "    return stemmed\n",
    "\n",
    "with open(\"../data_set/algerian_arabic_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(word.strip() for word in f.readlines())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word not in custom_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: محمد صلاح يقود نادي ليفربول للفوز الكبير 🏆 على مانشستر يونايتد\n",
      "Normalized Text: محمد صلاح يقود نادي ليفربول للفوز الكبير  مانشستر يونايتد\n",
      "Stemmed Text: محمد صلاح قاد نادي ليفربول فوز كبير مانشستر يونايتد\n",
      "POS Tags (Split):\n",
      "S/S\n",
      "محمد/NOUN-MS\n",
      "صلاح/NOUN-MS\n",
      "قاد/V\n",
      "نادي/NOUN-MS\n",
      "ليفربول/NOUN-MS\n",
      "فوز/NOUN-MS\n",
      "كبير/ADJ-MS\n",
      "مانشستر/NOUN-MS\n",
      "يونايتد/NOUN-MS\n",
      "E/E\n"
     ]
    }
   ],
   "source": [
    "text =\"محمد صلاح يقود نادي ليفربول للفوز الكبير 🏆 على مانشستر يونايتد\"\n",
    "\n",
    "#ليفربول #محمد_صلاح #الدوري_الإنجليزي\"\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "\n",
    "# Remove stopwords from the text\n",
    "text_without_stopwords = remove_stopwords(text)\n",
    "\n",
    "# Normalize the text after removing stopwords\n",
    "normalized_text = normalize_arabic(text_without_stopwords)\n",
    "print(\"Normalized Text:\", normalized_text)\n",
    "\n",
    "stemmed_text = stemmer.stem(normalized_text)\n",
    "print(\"Stemmed Text:\", stemmed_text)\n",
    "\n",
    "# Now POS tagging on stemmed text\n",
    "pos_tags_split = pos_tagger.tag(stemmed_text).split()\n",
    "print(\"POS Tags (Split):\")\n",
    "for tag in pos_tags_split:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: محمد صلاح يقود نادي ليفربول للفوز الكبير 🏆 على مانشستر يونايتد\n",
      "Normalized Text: محمد صلاح يقود نادي ليفربول للفوز الكبير  علي مانشستر يونايتد\n",
      "Stemmed Text: محمد صلاح قاد نادي ليفربول فوز كبير علي مانشستر يونايتد\n",
      "\n",
      "Normalized Text as Words: ['محمد', 'صلاح', 'يقود', 'نادي', 'ليفربول', 'للفوز', 'الكبير', 'علي', 'مانشستر', 'يونايتد']\n",
      "\n",
      "After processing: ['محمد', 'صلاح', 'قاد', 'نادي', 'ليفربول', 'فوز', 'كبير', 'علي', 'مانشستر', 'يونايتد']\n"
     ]
    }
   ],
   "source": [
    "# Your input text\n",
    "text = \"محمد صلاح يقود نادي ليفربول للفوز الكبير 🏆 على مانشستر يونايتد\"\n",
    "\n",
    "# Print the original text\n",
    "print(\"Original Text:\", text)\n",
    "\n",
    "# Normalize the text\n",
    "normalized_text = normalize_arabic(text)\n",
    "print(\"Normalized Text:\", normalized_text)\n",
    "\n",
    "# Stem the normalized text\n",
    "stemmed_text = stemmer.stem(normalized_text)\n",
    "print(\"Stemmed Text:\", stemmed_text)\n",
    "\n",
    "# Function to split text into words\n",
    "def split_text_to_words(text):\n",
    "    # Split the text by spaces or punctuation\n",
    "    return [word for word in text.split() if word not in ['.', ',', '!', '?', '…', '(', ')']]\n",
    "\n",
    "# Split normalized and stemmed text into words\n",
    "normalized_words = split_text_to_words(normalized_text)\n",
    "stemmed_words = split_text_to_words(stemmed_text)\n",
    "\n",
    "# Output the word lists\n",
    "print(\"\\nNormalized Text as Words:\", normalized_words)\n",
    "print(\"\\nAfter processing:\", stemmed_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
